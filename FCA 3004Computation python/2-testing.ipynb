{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Yanjun Zhang\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6273326574ec9693a7f67a3679389d2e",
     "grade": false,
     "grade_id": "cell-c374456facd20e46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Testing\n",
    "\n",
    "Testing is one of the most important components of sustainable software development. It improves code quality, maintainability and lifetime, and saves developer time. In previous labs you have already come across this in the form of assert statements. There are a number of testing libraries to support Python development and they all have in common that they build on the assert statement. In addition they provide better information when errors are found and the facilitate automated testing of large projects.\n",
    "\n",
    "We will use the doctest and the pytest libraries. If pytest is not installed you need to install it with pip. On your computers do this in the virtual environment where you previously installed jupyter\n",
    "\n",
    "~~~\n",
    "$ pip install pytest\n",
    "$ jupyter notebook\n",
    "~~~\n",
    "\n",
    "Testing libraries are typically designed to be used on Python source files while they are not adapted to be used with Jupyter notebooks. To be able to work with this in this lab, we use cell magic commands (%%file) to save cells in ordinary files and then execute pytest on those files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b419f62e162c6a542eeacf8bdb910bba",
     "grade": false,
     "grade_id": "cell-33440c27e09b63e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import doctest\n",
    "import pytest\n",
    "# This is for jupyter to recognize changes in external files without restarting kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: add time stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2655a28b70be86fdb6d905e6ca8e95b",
     "grade": false,
     "grade_id": "cell-01f525753e8b1889",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting timestamps.py\n"
     ]
    }
   ],
   "source": [
    "%%file timestamps.py\n",
    "from timestamps1 import timestamp_to_seconds\n",
    "from timestamps2 import seconds_to_timestamp\n",
    "\n",
    "def sum_timestamps(l):\n",
    "    \"\"\"\n",
    "    >>> sum_timestamps(['5:32', '4:48'])\n",
    "    '10:20'\n",
    "    >>> sum_timestamps(['03:10', '01:00'])\n",
    "    '4:10'\n",
    "    >>> sum_timestamps(['2:10', '1:59'])\n",
    "    '4:09'\n",
    "    >>> sum_timestamps(['15:32', '45:48'])\n",
    "    '1:01:20'\n",
    "    >>> sum_timestamps(['6:15:32', '2:45:48'])\n",
    "    '9:01:20'\n",
    "    >>> sum_timestamps(['6:35:32', '2:45:48', '40:10'])\n",
    "    '10:01:30'\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for ts in l:    \n",
    "        total += timestamp_to_seconds(ts)\n",
    "        \n",
    "    total_as_timestamp = seconds_to_timestamp(total)\n",
    "    return total_as_timestamp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9deedf0b2db2f0be30c26e3a81a55162",
     "grade": false,
     "grade_id": "cell-0ea01ce4cc34e8ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting timestamps1.py\n"
     ]
    }
   ],
   "source": [
    "%%file timestamps1.py\n",
    "def timestamp_to_seconds(tst):\n",
    "    \"\"\"\n",
    "    >>> timestamp_to_seconds(\"1:01\")\n",
    "    61\n",
    "    >>> timestamp_to_seconds(\"1:00:00\")\n",
    "    3600\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    ################\n",
    "    \n",
    "    total = 0\n",
    "    ts=tst.split(\":\")\n",
    "    \n",
    "    if   len(ts) == 1:\n",
    "        total = int(ts[0])\n",
    "    elif len(ts) == 2:\n",
    "        total = int(ts[0])*60+int(ts[1])\n",
    "    elif len(ts) == 3:\n",
    "        total = int(ts[0])*3600+int(ts[1])*60+int(ts[2])\n",
    "      \n",
    "    return total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7384\n"
     ]
    }
   ],
   "source": [
    "from timestamps1 import timestamp_to_seconds\n",
    "c = timestamp_to_seconds(\"2:3:4\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b894b419585c397b266e9d2c3c27f3a2",
     "grade": true,
     "grade_id": "cell-1b37e8d4ce5a07e1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timestamps1\n",
    "doctest.testmod(timestamps1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a75a15cadc9567e0852505080310e63f",
     "grade": false,
     "grade_id": "cell-79d2018d1ab06c44",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting timestamps2.py\n"
     ]
    }
   ],
   "source": [
    "%%file timestamps2.py\n",
    "\n",
    "def seconds_to_timestamp(seconds):\n",
    "    \"\"\"\n",
    "    >>> seconds_to_timestamp(61)\n",
    "    '1:01'\n",
    "    >>> seconds_to_timestamp(3600)\n",
    "    '1:00:00'\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    ################\n",
    "         \n",
    "    hour = int(seconds/3600)\n",
    "    minutes = int((seconds - hour*3600)/60)\n",
    "    secondss = int(seconds - (hour*3600) - (minutes*60))\n",
    "            \n",
    "      \n",
    "    if hour == 0:\n",
    "        minutes = str(int(minutes)).zfill(1)\n",
    "        secondss = str(int(secondss)).zfill(2)\n",
    "        time = ':'.join([minutes,secondss])  \n",
    "        return time \n",
    "    \n",
    "    else:\n",
    "        hour = str(int(hour))\n",
    "        minutes = str(int(minutes)).zfill(2)\n",
    "        secondss = str(int(secondss)).zfill(2)\n",
    "        time = ':'.join([hour,minutes,secondss])  \n",
    "        return time \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26:01\n"
     ]
    }
   ],
   "source": [
    "import timestamps2\n",
    "c = seconds_to_timestamp(1561)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b3f9f326997c0ac4a76e69a6df58b4a",
     "grade": true,
     "grade_id": "cell-210600fd395cba29",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timestamps2\n",
    "from timestamps2 import *\n",
    "doctest.testmod(timestamps2, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: time_stamps with pytest\n",
    "\n",
    "Write a test file to be used with pytest for assignment 1, with the same test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f15f003d3deb585ca2cbcb1bba918eea",
     "grade": false,
     "grade_id": "cell-ed7cd15078053e50",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_timestamps.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_timestamps.py\n",
    "import timestamps\n",
    "\n",
    "# YOUR CODE HERE\n",
    "################\n",
    "def test_1():\n",
    "    assert timestamps.timestamp_to_seconds(\"1:01\") == 61\n",
    "    assert timestamps.timestamp_to_seconds(\"1:00:00\") == 3600\n",
    "    \n",
    "def test_seconds_to_timesttamps():\n",
    "    assert timestamps.seconds_to_timestamp(61) == (\"1:01\")\n",
    "    assert timestamps.seconds_to_timestamp(3600) == (\"1:00:00\")\n",
    "    \n",
    "def test_sum_timestamps():\n",
    "    assert timestamps.sum_timestamps(['5:32', '4:48']) == '10:20'\n",
    "    assert timestamps.sum_timestamps(['03:10', '01:00']) == '4:10'\n",
    "    assert timestamps.sum_timestamps(['2:10', '1:59']) == '4:09'\n",
    "    assert timestamps.sum_timestamps(['15:32', '45:48']) == '1:01:20'\n",
    "    assert timestamps.sum_timestamps(['6:15:32', '2:45:48']) == '9:01:20'\n",
    "    assert timestamps.sum_timestamps(['6:35:32', '2:45:48', '40:10']) == '10:01:30'\n",
    "    # Add more test cases as needed\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee29c06d79b8bcedd807baa0cfac3473",
     "grade": true,
     "grade_id": "cell-c597b267d405be1f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m================================================= test session starts =================================================\u001b[0m\n",
      "platform win32 -- Python 3.12.0, pytest-7.4.3, pluggy-1.3.0 -- C:\\Python\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\Yanjun\\OneDrive - KTH\\Course\\FCA 3004 Computational Python\\exercise\n",
      "plugins: anyio-4.0.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "test_timestamps.py::test_1 \u001b[32mPASSED\u001b[0m\u001b[32m                                                                                [ 33%]\u001b[0m\n",
      "test_timestamps.py::test_seconds_to_timesttamps \u001b[32mPASSED\u001b[0m\u001b[32m                                                           [ 66%]\u001b[0m\n",
      "test_timestamps.py::test_sum_timestamps \u001b[32mPASSED\u001b[0m\u001b[32m                                                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ==================================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest.main(['-v', 'test_timestamps.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aff4a31a576f7a13a7f174e005cf620b",
     "grade": false,
     "grade_id": "cell-a4c4bdbc6f98dac1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 3: running mean\n",
    "\n",
    "This is an example use of parametrized test cases. Look at the test function and understand how it works.\n",
    "Write a function that passes the tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f15c3812d0a4c64c65dfe19947283f0b",
     "grade": false,
     "grade_id": "cell-a62f5af5275891d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_running_mean.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_running_mean.py\n",
    "import pytest\n",
    "\n",
    "from running_mean import running_mean\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"input_argument, expected_return\", [\n",
    "    ([1, 2, 3], [1, 1.5, 2]),\n",
    "    ([2, 6, 10, 8, 11, 10],\n",
    "     [2.0, 4.0, 6.0, 6.5, 7.4, 7.83]),\n",
    "    ([3, 4, 6, 2, 1, 9, 0, 7, 5, 8],\n",
    "     [3.0, 3.5, 4.33, 3.75, 3.2, 4.17, 3.57, 4.0, 4.11, 4.5]),\n",
    "    ([], []),\n",
    "])\n",
    "def test_running_mean(input_argument, expected_return):\n",
    "    ret = list(running_mean(input_argument))\n",
    "    assert ret == expected_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "772c3fe89217aea5bc44c3753181dbbe",
     "grade": false,
     "grade_id": "cell-96c10ee58ecac500",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting running_mean.py\n"
     ]
    }
   ],
   "source": [
    "%%file running_mean.py\n",
    "def running_mean(sequence,decimal_places =2):\n",
    "    \"\"\"Calculate the running mean of the sequence passed in,\n",
    "       returns a sequence of same length with the averages.\n",
    "       You can assume all items in sequence are numeric.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    ################\n",
    "\n",
    "    sum_so_far = 0\n",
    "    running_means = []\n",
    "       \n",
    "    for i, value in enumerate(sequence,1):\n",
    "        sum_so_far += value\n",
    "        running_mean = round(sum_so_far / i, decimal_places)\n",
    "        \n",
    "        running_means.append(running_mean)\n",
    "        \n",
    "    return running_means\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.5, 2.0]\n"
     ]
    }
   ],
   "source": [
    "from running_mean import running_mean\n",
    "f = running_mean([1,2,3])\n",
    "print (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2214c633a7a294cb33de7bbac7d2bad5",
     "grade": true,
     "grade_id": "cell-6a057ac659530fd9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m================================================= test session starts =================================================\u001b[0m\n",
      "platform win32 -- Python 3.12.0, pytest-7.4.3, pluggy-1.3.0 -- C:\\Python\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\Yanjun\\OneDrive - KTH\\Course\\FCA 3004 Computational Python\\exercise\n",
      "plugins: anyio-4.0.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "test_running_mean.py::test_running_mean[input_argument0-expected_return0] \u001b[32mPASSED\u001b[0m\u001b[32m                                 [ 25%]\u001b[0m\n",
      "test_running_mean.py::test_running_mean[input_argument1-expected_return1] \u001b[32mPASSED\u001b[0m\u001b[32m                                 [ 50%]\u001b[0m\n",
      "test_running_mean.py::test_running_mean[input_argument2-expected_return2] \u001b[32mPASSED\u001b[0m\u001b[32m                                 [ 75%]\u001b[0m\n",
      "test_running_mean.py::test_running_mean[input_argument3-expected_return3] \u001b[32mPASSED\u001b[0m\u001b[32m                                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================================================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ==================================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest.main(['-v', 'test_running_mean.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "\n",
    "Write a second version of the test function for timestamp where the different test cases are different parameterizations for one test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55d154d0765d0fe91a1a7634cbf0076e",
     "grade": false,
     "grade_id": "cell-1d068aaf56835af8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_timestamps_parametrized.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_timestamps_parametrized.py\n",
    "import pytest\n",
    "\n",
    "from timestamps import sum_timestamps\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "################\n",
    "\n",
    "@pytest.mark.parametrize(\"input_argument, expected_return\", [\n",
    "    (['5:32', '4:48'],str('10:20')),\n",
    "    (['2:10', '1:59'],str('4:09')),\n",
    "  \n",
    "    \n",
    "  ])\n",
    "\n",
    "def test_running_mean(input_argument, expected_return):\n",
    "    ret = sum_timestamps(input_argument)\n",
    "    assert ret == expected_return\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8b2d982b43e73f1cb7e07d5806071ea",
     "grade": true,
     "grade_id": "cell-32b906546d801ca5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m================================================= test session starts =================================================\u001b[0m\n",
      "platform win32 -- Python 3.12.0, pytest-7.4.3, pluggy-1.3.0 -- C:\\Python\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\Yanjun\\OneDrive - KTH\\Course\\FCA 3004 Computational Python\\exercise\n",
      "plugins: anyio-4.0.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_timestamps_parametrized.py::test_running_mean[input_argument0-10:20] \u001b[32mPASSED\u001b[0m\u001b[32m                                 [ 50%]\u001b[0m\n",
      "test_timestamps_parametrized.py::test_running_mean[input_argument1-4:09] \u001b[32mPASSED\u001b[0m\u001b[32m                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ==================================================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExitCode.OK: 0>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest.main(['-v', 'test_timestamps_parametrized.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
